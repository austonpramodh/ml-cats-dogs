{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Size POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import sys\n",
    "\n",
    "def read_file_into_memory(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        file_content = file.read()\n",
    "    return file_content\n",
    "\n",
    "# Example usage\n",
    "file_path = \"/home/austonpramodh/Desktop/Projects/ml-cats-dogs/model-building/cats-dogs/data/PetImages/Dog/12443.jpg\"\n",
    "file_content = read_file_into_memory(file_path)\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Assuming file_content is a binary string containing the JPEG image data\n",
    "# First, convert the binary data into a PIL image\n",
    "image = Image.open(io.BytesIO(file_content))\n",
    "\n",
    "image2 = torch.ByteTensor(torch.ByteStorage.from_buffer(image.tobytes()))\n",
    "\n",
    "# Then, convert the PIL image into a tensor\n",
    "image = T.PILToTensor()(image)\n",
    "\n",
    "image = T.Grayscale()(image)\n",
    "\n",
    "\n",
    "print(\"CPU Image Input Tensor - \", image.shape, image.dtype)\n",
    "\n",
    "print(\"size of the file in RAW Bytes in KB\", sys.getsizeof(file_content)/1024)\n",
    "\n",
    "calculate_tensor_size = lambda x: sys.getsizeof(x) + torch.numel(x)*x.element_size()\n",
    "\n",
    "print(\"size of the file in Tensor Bytes in KB\", calculate_tensor_size(image)/1024)\n",
    "\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "print(image2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install numpy pandas Pillow matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "import subprocess\n",
    "\n",
    "# if the platform is Mac OS X, install the nightly build of PyTorch\n",
    "if platform == \"darwin\":\n",
    "  print(\"Installing nightly build of PyTorch for Mac OS X\")\n",
    "  # %pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "  subprocess.run([\"pip\", \"install\", \"--pre\", \"torch\", \"torchvision\", \"torchaudio\", \"--extra-index-url\", \"https://download.pytorch.org/whl/nightly/cpu\"])\n",
    "elif platform == \"win32\":\n",
    "  print(\"Installing PyTorch for Windows\")\n",
    "  # %pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/torch_stable.html\n",
    "  subprocess.run([\"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cu117\"])\n",
    "# if the platform is Linux, install the nightly build of PyTorch\n",
    "else:\n",
    "  print(\"Installing PyTorch for Linux\")\n",
    "  # %pip install torch torchvision torchaudio\n",
    "  subprocess.run([\"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from matplotlib import image, pyplot\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use GPU if it exists - for faster execution\n",
    "# Device selection for PyTorch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Device selection for PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print('Using GPU')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print('Using MPS')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file already exists\n",
      "The file is already extracted\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset and unzip - RUN ONLY ONCE!\n",
    "# https://www.microsoft.com/en-us/download/details.aspx?id=54765\n",
    "zip_file_url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\"\n",
    "import requests, zipfile, sys\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def download_file(url, save_path = \"\", forceDownload=False):\n",
    "    # Check if the file already exists\n",
    "    if save_path == \"\":\n",
    "        save_path = url.split('/')[-1]\n",
    "        save_path = os.path.join(os.getcwd(), \"data\", save_path)\n",
    "\n",
    "    if os.path.exists(save_path) and not forceDownload:\n",
    "        print(\"The file already exists\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading the file from {url} to {save_path}\")\n",
    " \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 Kibibyte\n",
    "    \n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(save_path, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "    \n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR: Download incomplete\")\n",
    "    else:\n",
    "        print(\"Download complete\")\n",
    "\n",
    "# Check if the data folder exists, if not, create it\n",
    "import os\n",
    "if not os.path.exists(\"./data/\"):\n",
    "    os.makedirs(\"./data/\")\n",
    "\n",
    "local_filename = zip_file_url.split('/')[-1]\n",
    "download_file(zip_file_url)\n",
    "# unzip the file\n",
    "# Get the names of the folders in the zip file\n",
    "with zipfile.ZipFile(\"./data/\" + local_filename, 'r') as zip_ref:\n",
    "    # Get first level folders\n",
    "    folders = set([os.path.dirname(name) for name in zip_ref.namelist()])\n",
    "    # Check if the folders already exists\n",
    "    extracted_already = True\n",
    "    folders = [os.path.join(\"./data/\", folder) for folder in folders]\n",
    "    for folder in folders:\n",
    "        if not os.path.exists(folder):\n",
    "            extracted_already = False\n",
    "            break\n",
    "    if not extracted_already:\n",
    "        print(\"Unzipping the file\")\n",
    "        zip_ref.extractall(\"./data/\")\n",
    "    else:\n",
    "        print(\"The file is already extracted\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all the images into the memory\n",
    "# Lets check how many files are in the dataset\n",
    "import os\n",
    "from typing import List\n",
    "from matplotlib import image\n",
    "\n",
    "\n",
    "def getAllFiles(categories: List[str], data_root: str, includeFileData=False) -> dict:\n",
    "  filesByCategory = {}\n",
    "\n",
    "  for category in categories:\n",
    "    count = 0\n",
    "    full_path = data_root + category\n",
    "\n",
    "    for file_name in os.listdir(full_path): \n",
    "      # Check if the file is an image\n",
    "      if not file_name.endswith(\".jpg\") and not file_name.endswith(\".jpeg\") and not file_name.endswith(\".png\"):\n",
    "        continue\n",
    "\n",
    "      count+=1\n",
    "      file_path = full_path + \"/\" + file_name\n",
    "      file = {\n",
    "        \"file_name\": file_name,\n",
    "        \"file_path\": file_path,\n",
    "        \"data\": None\n",
    "      }\n",
    "      \n",
    "      if includeFileData:\n",
    "        file[\"data\"] = image.imread(file_path)\n",
    "        \n",
    "      if category in filesByCategory.keys():\n",
    "        filesByCategory[category].append(file)\n",
    "      else:\n",
    "        filesByCategory[category] = [file]\n",
    "\n",
    "    print(f\"Total {category} images: {count}\")\n",
    "  return filesByCategory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Cat\", \"Dog\"]\n",
    "\n",
    "class LabelTransformer():\n",
    "    labels_map = {}\n",
    "    labels_id_map = {}\n",
    "    \n",
    "    def __init__(self, labels):\n",
    "        # Create a labelMap\n",
    "        labels_set = set(labels)\n",
    "\n",
    "        for id,val in enumerate(labels_set):\n",
    "            self.labels_map[val] = id\n",
    "            self.labels_id_map[id] = val\n",
    "\n",
    "    def encoder(self, label):\n",
    "        return self.labels_map[label]\n",
    "\n",
    "    def decoder(self, label_encoded):\n",
    "        return self.labels_id_map[label_encoded]\n",
    "\n",
    "label_transformer = LabelTransformer(classes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), \"data/PetImages/\")\n",
    "filesByCategory = getAllFiles(classes, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if all images are readable\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# for category in filesByCategory.keys():\n",
    "#   for file in filesByCategory[category]:\n",
    "#     # if file[\"data\"] is None:\n",
    "#         try:\n",
    "#             # file[\"data\"] = \n",
    "#             # image.imread(file[\"file_path\"])\n",
    "#             # Need to fix this reading error, by using image.imread instead of read_image from torchvision\n",
    "#             read_image(file[\"file_path\"])\n",
    "#         except:\n",
    "#             print(f\"Error reading file {file['file_path']}\")\n",
    "#             filesByCategory[category].remove(file)\n",
    "\n",
    "# print(len(filesByCategory[\"Cat\"]))\n",
    "# print(len(filesByCategory[\"Dog\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for category in filesByCategory:\n",
    "  print(f\"------{category}!----\")\n",
    "  file_path = filesByCategory[category][0][\"file_path\"]\n",
    "  my_image = image.imread(file_path)\n",
    "  print(my_image.dtype)\n",
    "  print(my_image.shape)\n",
    "  pyplot.imshow(my_image)\n",
    "  pyplot.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create X Train and y Train\n",
    "files_by_category = filesByCategory\n",
    "train_XY = []\n",
    "validation_XY = []\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "class ImageLabelData():\n",
    "    def __init__(self, path, label):\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "\n",
    "for i in range(0, len(classes)):\n",
    "  category = classes[i]\n",
    "  files = files_by_category[category]\n",
    "  random.shuffle(files)\n",
    "  split_index = int(len(files) * VALIDATION_SPLIT)\n",
    "  training_files = files[split_index:]\n",
    "  validation_files = files[:split_index]\n",
    "\n",
    "  for file in training_files:\n",
    "    # X_train.append(file[\"file_path\"])\n",
    "    # y_train.append(category)\n",
    "    train_XY.append(ImageLabelData(file[\"file_path\"], category))\n",
    "  \n",
    "  for file in validation_files:\n",
    "    validation_XY.append(ImageLabelData(file[\"file_path\"], category))\n",
    "\n",
    "# Random shuffle the data\n",
    "random.shuffle(train_XY)\n",
    "random.shuffle(validation_XY)\n",
    "\n",
    "print(f\"Total Training Images: {len(train_XY)}\")\n",
    "print(f\"Total Validation Images: {len(validation_XY)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Class for DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Max Memory Available\n",
    "import psutil\n",
    "import humanize\n",
    "import sys\n",
    "\n",
    "\n",
    "class MemoryCache():\n",
    "    # static variable for holding the instance reference\n",
    "    __instances = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_stats():\n",
    "        for key in MemoryCache.__instances:\n",
    "            print(MemoryCache.__instances[key])\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear():\n",
    "        for key in MemoryCache.__instances:\n",
    "            del MemoryCache.__instances[key]\n",
    "        print(\"Memory Cache Cleared\")\n",
    "\n",
    "    # static method for getings instance\n",
    "    @staticmethod\n",
    "    def getInstance(max_memory=None, usable_memory_percentage=0.6, DEBUG=False, value_size_getter=None, instance_name=\"Generic\"):\n",
    "        # check if instance witg the same parameters already exists\n",
    "        key = instance_name\n",
    "        if key not in MemoryCache.__instances:\n",
    "            # if not, create a new instance\n",
    "            MemoryCache.__instances[key] = MemoryCache(\n",
    "                max_memory, usable_memory_percentage, DEBUG, value_size_getter, instance_name)\n",
    "        # return the instance\n",
    "        return MemoryCache.__instances[key]\n",
    "\n",
    "    def print(self, *args, **kwargs):\n",
    "        if self.DEBUG:\n",
    "            print(*args, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Name: {self.instance_name}, Max Memory: {humanize.naturalsize(self.max_memory)}, Current Memory: {humanize.naturalsize(self.get_size())}, Entries: {len(self.img_cache)}\"\n",
    "\n",
    "    def __init__(self, max_memory=None, usable_memory_percentage=0.6, DEBUG=False, value_size_getter=None, instance_name=None):\n",
    "        self.DEBUG = DEBUG\n",
    "        self.print(\"Initializing Memory Cache - \", instance_name if instance_name is not None else \"No Name!\")\n",
    "\n",
    "        self.instance_name = instance_name\n",
    "\n",
    "        self.value_size_getter = value_size_getter\n",
    "\n",
    "        if max_memory is None:\n",
    "            mem = psutil.virtual_memory()\n",
    "            # Get the max memory available for the images\n",
    "            max_memory = mem.available * usable_memory_percentage\n",
    "        self.max_memory = max_memory\n",
    "        self.img_cache = {}\n",
    "\n",
    "        self.print(\"Memory Cache Initialized\")\n",
    "\n",
    "        self.print(f\"Max Memory: {humanize.naturalsize(self.max_memory)}\")\n",
    "\n",
    "    def get_size(self, is_verbose=None):\n",
    "        is_verbose = is_verbose if is_verbose is not None else self.DEBUG\n",
    "        size = 0\n",
    "        for key in self.img_cache:\n",
    "            size += sys.getsizeof(key)\n",
    "            if self.value_size_getter is not None:\n",
    "                size += self.value_size_getter(self.img_cache[key])\n",
    "            else:\n",
    "                size += sys.getsizeof(self.img_cache[key])\n",
    "            if is_verbose:\n",
    "                self.print(\n",
    "                    f\"Current Memory Used: {humanize.naturalsize(size)}, Max Memory: {humanize.naturalsize(self.max_memory)}, Memory Used: {size/self.max_memory*100:.3f}%\")\n",
    "        return size\n",
    "\n",
    "    def can_fit_in_memory(self, is_verbose=None):\n",
    "        is_verbose = is_verbose if is_verbose is not None else self.DEBUG\n",
    "\n",
    "        obj_size = self.get_size(is_verbose=False)\n",
    "        if is_verbose:\n",
    "            self.print(f\"Object Size: {humanize.naturalsize(obj_size)} - Max Memory: {humanize.naturalsize(self.max_memory)}\")\n",
    "        \n",
    "        return obj_size < self.max_memory\n",
    "\n",
    "    def set(self, key, cache):\n",
    "        cache_size = None\n",
    "        if self.value_size_getter is not None:\n",
    "            cache_size = self.value_size_getter(cache)\n",
    "        else:\n",
    "            cache_size = sys.getsizeof(cache)\n",
    "\n",
    "        if self.can_fit_in_memory(is_verbose=False):\n",
    "            self.img_cache[key] = cache\n",
    "            self.print(f\"Image Cached: {key} - Size: {humanize.naturalsize(cache_size)}\")\n",
    "        else:\n",
    "            self.print(\"Not enough memory to cache the image\")\n",
    "\n",
    "        memory_used = self.get_size(is_verbose=False) / self.max_memory * 100\n",
    "        self.print(\n",
    "            f'Data size: {humanize.naturalsize(cache_size)} - Cache size: {humanize.naturalsize(self.get_size(is_verbose=False))} - Memory Used: {memory_used:.3f}% - Entries: {len(self.img_cache)}')\n",
    "\n",
    "    def get(self, key):\n",
    "        if key in self.img_cache:\n",
    "            return self.img_cache[key]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def exists(self, key):\n",
    "        return key in self.img_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset for DataLoader\n",
    "\n",
    "torch.manual_seed(17)\n",
    "\n",
    "\n",
    "class CustomImageDatasetV2(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, pre_cache_transform = None, transform=None, target_transform=None,\n",
    "                 max_cache_memory_available=None, usable_cache_memory_percentage=0.6, memory_cache_instance_name=None):\n",
    "        self.dataset = dataset\n",
    "        self.pre_cache_transform = pre_cache_transform\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        # Since x is a tensor, we need to use the storage() method to get the size of the tensor\n",
    "        def value_size_getter(x): return sys.getsizeof(x) + torch.numel(x)*x.element_size()\n",
    "        # def value_size_getter(x): return sys.getsizeof(x)\n",
    "        self.memory_cache = MemoryCache.getInstance(max_memory=max_cache_memory_available,  usable_memory_percentage=usable_cache_memory_percentage,\n",
    "                                                    value_size_getter=value_size_getter, DEBUG=False, instance_name=memory_cache_instance_name)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __read_file_into_memory(self, file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_content = file.read()\n",
    "        return file_content\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        label = self.dataset[idx].label\n",
    "        img_path = self.dataset[idx].path\n",
    "\n",
    "        image = None\n",
    "        if self.memory_cache.exists(img_path):\n",
    "            image = self.memory_cache.get(img_path)\n",
    "        else:\n",
    "            # image = self.__read_file_into_memory(img_path)\n",
    "            image = read_image(img_path)\n",
    "\n",
    "            if self.pre_cache_transform:\n",
    "                image = self.pre_cache_transform(image)\n",
    "\n",
    "            self.memory_cache.set(img_path, image)\n",
    "\n",
    "        # Image transformation\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Label transformation\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the dataset for DataLoader\n",
    "from torchvision.io import read_image\n",
    "import time\n",
    "\n",
    "torch.manual_seed(17)\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, target_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.dataset[idx].label\n",
    "        img_path = self.dataset[idx].path\n",
    "        \n",
    "        image = read_image(img_path)        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class LambdaModule(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super().__init__()\n",
    "        import types\n",
    "        assert type(lambd) is types.LambdaType\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "# Create the DataLoader with some transformations\n",
    "image_size = 224\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "# Image transformations\n",
    "image_transforms = {\n",
    "    'precache':\n",
    "    torch.nn.Sequential(\n",
    "        T.Grayscale(),\n",
    "    ),\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    torch.nn.Sequential(\n",
    "        LambdaModule(lambda x: x.to(device)),\n",
    "        # T.ToPILImage(),\n",
    "        # T.Resize((image_size, image_size)),\n",
    "        T.RandomResizedCrop(image_size),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        # LambdaModule(lambda x: x.squeeze()),\n",
    "        T.ConvertImageDtype(torch.float),\n",
    "    ),\n",
    "    # Validation does not use augmentation\n",
    "    'val':\n",
    "    torch.nn.Sequential(\n",
    "        LambdaModule(lambda x: x.to(device)),\n",
    "        T.Resize((image_size, image_size)),\n",
    "        # LambdaModule(lambda x: x.squeeze()),\n",
    "        T.ConvertImageDtype(torch.float),\n",
    "    ),\n",
    "    # Test does not use augmentation\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize((image_size, image_size)),\n",
    "        T.Grayscale(),\n",
    "        T.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test transformer on all images since its throwing an error\n",
    "\n",
    "# i = 0\n",
    "# while len(train_XY) > i:\n",
    "#     try:\n",
    "#         image = read_image(train_XY[i].path)\n",
    "#         image_transforms[\"train\"](image)\n",
    "#         image_transforms[\"val\"](image)\n",
    "#         image_transforms[\"test\"](image)\n",
    "#         i += 1\n",
    "#     except:\n",
    "#         print(f\"Error on image {train_XY[i].path}\")\n",
    "#         # Remove the image from the training set\n",
    "#         train_XY.pop(i)\n",
    "\n",
    "# # Test transformer on all images since its throwing an error\n",
    "# i = 0\n",
    "# while len(validation_XY) > i:\n",
    "#     try:\n",
    "#         image = read_image(validation_XY[i].path)\n",
    "#         image_transforms[\"train\"](image)\n",
    "#         image_transforms[\"val\"](image)\n",
    "#         image_transforms[\"test\"](image)\n",
    "#         i += 1\n",
    "#     except:\n",
    "#         print(f\"Error on image {validation_XY[i].path}\")\n",
    "#         # Remove the image from the training set\n",
    "#         validation_XY.pop(i)\n",
    "\n",
    "# # Valid Images num\n",
    "# print(f\"Total Training Images: {len(train_XY)}\")\n",
    "# print(f\"Total Validation Images: {len(validation_XY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LambdaModule(nn.Module):\n",
    "#     def __init__(self, lambd):\n",
    "#         super().__init__()\n",
    "#         import types\n",
    "#         assert type(lambd) is types.LambdaType\n",
    "#         self.lambd = lambd\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.lambd(x)\n",
    "    \n",
    "# # Create the DataLoader with some transformations\n",
    "# image_size = 224\n",
    "# transform = T.Compose([\n",
    "#     T.ToPILImage(),\n",
    "#     T.Resize((image_size, image_size)),\n",
    "#     T.RandomResizedCrop(image_size),\n",
    "#     T.RandomHorizontalFlip(),\n",
    "#     T.Grayscale(),\n",
    "#     T.ToTensor(),\n",
    "#     # LambdaModule(lambda x: x.squeeze())\n",
    "# ])\n",
    "\n",
    "# gpu_available = torch.cuda.is_available()\n",
    "\n",
    "# transform_gpu = torch.nn.Sequential(\n",
    "#     # T.ToPILImage(),\n",
    "#     T.Resize((image_size, image_size)),\n",
    "#     T.RandomResizedCrop(image_size),\n",
    "#     T.RandomHorizontalFlip(),\n",
    "#     T.Grayscale(),\n",
    "#     # LambdaModule(lambda x: x.squeeze()),\n",
    "#     T.ConvertImageDtype(torch.float),\n",
    "# )\n",
    "\n",
    "\n",
    "# # Testing the Transformer - CPU\n",
    "# my_image = read_image(train_XY[0].path)\n",
    "# print(\"CPU Image Input - \", my_image.shape, my_image.dtype)\n",
    "# print(\"CPU Image Input - \", my_image)\n",
    "# transformed_image = transform(my_image)\n",
    "# print(\"CPU Image Output - \", transformed_image)\n",
    "# print(\"CPU Image Output - \", transformed_image.shape, transformed_image.dtype)\n",
    "# # pyplot.imshow(transformed_image.squeeze(), cmap=\"gray\")\n",
    "\n",
    "# # Testing the Transformer - GPU\n",
    "# my_image_gpu = read_image(train_XY[0].path)\n",
    "# print(\"GPU Image Input - \", my_image_gpu.shape, my_image_gpu.dtype)\n",
    "# my_image_gpu = my_image.to(device)\n",
    "# transformed_image_gpu = transform_gpu(my_image_gpu)\n",
    "\n",
    "\n",
    "# # # # Show the transformed image\n",
    "# gpu_cpu_img = transformed_image_gpu.squeeze().cpu()\n",
    "# pyplot.imshow(gpu_cpu_img, cmap=\"gray\")\n",
    "# print(\"GPU Image Output - \", transformed_image_gpu)\n",
    "# print(\"GPU Image Output - \", transformed_image_gpu.shape, transformed_image_gpu.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "class FileBinaryToTensorTransformerWrapper():\n",
    "    transformer = None\n",
    "    def __init__(self, transformer, device = None):\n",
    "        self.transformer = transformer\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, *args, **kwds):\n",
    "        image = PILImage.open(io.BytesIO(args[0]))\n",
    "\n",
    "        pilTransform = T.PILToTensor()\n",
    "\n",
    "        transformedImage = pilTransform(image)\n",
    "\n",
    "        if self.device:\n",
    "            transformedImage = transformedImage.to(device)\n",
    "\n",
    "        return self.transformer(transformedImage)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_transformer = FileBinaryToTensorTransformerWrapper(image_transforms[\"train\"], device)\n",
    "train_transformer = image_transforms[\"train\"]\n",
    "# validation_transformer = FileBinaryToTensorTransformerWrapper(image_transforms[\"val\"], device)\n",
    "validation_transformer = image_transforms[\"val\"]\n",
    "\n",
    "train_dataset = CustomImageDatasetV2(\n",
    "    dataset=train_XY,\n",
    "    transform=train_transformer,\n",
    "    pre_cache_transform=image_transforms[\"precache\"],\n",
    "    target_transform=label_transformer.encoder,\n",
    "    usable_cache_memory_percentage=0.5,\n",
    "    memory_cache_instance_name=\"Train Memory Cache\")\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "validation_dataset = CustomImageDatasetV2(\n",
    "            dataset=validation_XY,\n",
    "            transform=validation_transformer, \n",
    "            pre_cache_transform=image_transforms[\"precache\"],\n",
    "            target_transform=label_transformer.encoder,\n",
    "            usable_cache_memory_percentage=0.2,\n",
    "            memory_cache_instance_name=\"Validation Memory Cache\")\n",
    "validation_dl = DataLoader(validation_dataset, batch_size, shuffle=True, num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MemoryCache.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from PIL import Image\n",
    "\n",
    "image, label = train_dl.dataset[0]\n",
    "# pilTransform = T.ToPILImage()\n",
    "# transformedImage = pilTransform(image)\n",
    "# display(transformedImage)\n",
    "# print(image)\n",
    "\n",
    "\n",
    "# Need to write new Transformer for binary data\n",
    "# image = Image.open(io.BytesIO(image))\n",
    "\n",
    "# transformerV2 = T.Compose([\n",
    "#     T.Resize((image_size, image_size)),\n",
    "#     T.RandomResizedCrop(image_size),\n",
    "#     T.RandomHorizontalFlip(),\n",
    "#     T.Grayscale(),\n",
    "#     T.PILToTensor(),\n",
    "# ])\n",
    "\n",
    "transformerV2 = torch.nn.Sequential(\n",
    "    T.Resize((image_size, image_size)),\n",
    "    T.RandomResizedCrop(image_size),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.Grayscale(),\n",
    "    T.ConvertImageDtype(torch.float),\n",
    ")\n",
    "\n",
    "# class FileBinaryToTensorTransformerWrapper():\n",
    "#     transformer = None\n",
    "#     def __init__(self, transformer, device = None):\n",
    "#         self.transformer = transformer\n",
    "#         self.device = device\n",
    "\n",
    "#     def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "#         image = Image.open(io.BytesIO(args[0]))\n",
    "\n",
    "#         pilTransform = T.PILToTensor()\n",
    "\n",
    "#         transformedImage = pilTransform(image)\n",
    "\n",
    "#         if self.device:\n",
    "#             transformedImage = transformedImage.to(device)\n",
    "\n",
    "#         return self.transformer(transformedImage)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# print(\"device\",device)\n",
    "# my_transformer = FileBinaryToTensorTransformerWrapper(transformerV2, device=device)\n",
    "# image = my_transformer(image)\n",
    "# print(image)\n",
    "\n",
    "image = image.squeeze().cpu()\n",
    "pyplot.imshow(image, cmap=\"gray\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def show_images(images, nmax=64):\n",
    "    fig, ax = pyplot.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    images = images.cpu()\n",
    "    ax.imshow(make_grid((images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "\n",
    "def show_batch(dl, nmax=64):\n",
    "    for batch in tqdm(dl, \"Loading dataset....\", unit=\"batch\"):\n",
    "        images, labels = batch\n",
    "        show_images(images, nmax)\n",
    "        break\n",
    "\n",
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craete a neural network from pytorch\n",
    "# https://www.kaggle.com/code/reukki/pytorch-cnn-tutorial-with-cats-and-dogs\n",
    "class Cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cnn,self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(3*3*64,10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(10,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Craete a neural network from pytorch\n",
    "# class Cnn(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Cnn,self).__init__()\n",
    "        \n",
    "#         self.layer1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=0, stride=2)\n",
    "#         self.bactchnorm = nn.BatchNorm2d(num_features=16)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "#         self.fc1 = nn.Linear(55 * 55 * 16, 2)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         # Layer 1\n",
    "#         out = self.layer1(x)\n",
    "#         out = self.bactchnorm(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.maxpool(out)\n",
    "#         # Layer 1 -----\n",
    "\n",
    "#         out = self.relu(out)\n",
    "#         out = out.view(out.size(0), out.size(1)*out.size(2)* out.size(3))\n",
    "#         out = self.fc1(out)\n",
    "#         return out\n",
    "\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cnn().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_loss = []\n",
    "val_acc_loss = []\n",
    "learning_rate = 0.001\n",
    "total_epochs = 20\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(params = model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_completed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch < epoch_completed:\n",
    "        print(f\"Skipping epoch {epoch+1}\")\n",
    "        continue\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "\n",
    "    # epoch_accuracy=0\n",
    "    # epoch_loss=0\n",
    "    total_loss = 0.0  # Initialize total training loss for the epoch\n",
    "    total_correct = 0\n",
    "    total_samples=0\n",
    "    \n",
    "    for batch in tqdm(train_dl, desc=\"Training\", unit=\"batch\"):\n",
    "        images, labels = batch\n",
    "\n",
    "        # Convert to the device we are using\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Get the output and loss\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backpropagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate the accuracy and loss for training\n",
    "        total_loss += loss.item() * images.size(0)  # Accumulate the training loss\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        # Check if preds === a\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    epoch_loss = total_loss / len(train_dl.dataset)\n",
    "    epoch_accuracy = total_correct / total_samples\n",
    "    train_acc_loss.append([epoch_accuracy, epoch_loss])\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}], Training Accuracy: {epoch_accuracy:.4f}, Training Loss: {epoch_loss:.4f}, Time: {t1-t0:.2f} seconds\")\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_loss_val = 0.0\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_dl, desc=\"Validating\", unit=\"batch\"):\n",
    "            images, labels = batch\n",
    "\n",
    "            # Convert to the device we are using\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get the output and loss\n",
    "            outputs = model(images)\n",
    "            loss_val = criterion(outputs, labels)\n",
    "            total_loss_val += loss_val.item() * images.size(0)  # Accumulate the validation loss\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_correct_val += (predicted == labels).sum().item()\n",
    "            total_samples_val += labels.size(0)\n",
    "\n",
    "    # Compute average validation loss for the epoch\n",
    "    epoch_loss_val = total_loss_val / len(validation_dl.dataset)\n",
    "    epoch_accuracy_val = total_correct_val / total_samples_val\n",
    "    \n",
    "    val_acc_loss.append([epoch_accuracy_val, epoch_loss_val])\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f'Epoch [{epoch+1}/{total_epochs}], Validation Loss: {epoch_loss_val:.4f}, Validation Accuracy: {epoch_accuracy_val:.4f}, Time: {t1-t0:.2f} seconds')\n",
    "    print(\"=====================================================================================================\")\n",
    "    MemoryCache.print_stats()\n",
    "    print(\"=====================================================================================================\")\n",
    "\n",
    "    epoch_completed = epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MemoryCache.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a plot for accuracy against each epoch\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1,\n",
    "                                        ncols=2,\n",
    "                                        figsize=(20, 10))\n",
    "# Training -----\n",
    "# Train Accuracy per epoch\n",
    "y = [acc for acc, loss in train_acc_loss]\n",
    "x = np.arange(len(y))\n",
    "ax1.plot(x, y, color='g', label='training Accuracy')\n",
    "\n",
    "\n",
    "# Train Loss per epoch\n",
    "y = [loss for acc, loss in train_acc_loss]\n",
    "ax1.plot(x, y, color='b', label='trainig loss')\n",
    "\n",
    "\n",
    "# Validation -----\n",
    "# Validation Accuracy per epoch\n",
    "y = [acc for acc, loss in val_acc_loss]\n",
    "ax2.plot(x, y, color='r', label='validation Accuracy')\n",
    "\n",
    "# Validation loss per epoch\n",
    "y = [loss for acc, loss in val_acc_loss]\n",
    "ax2.plot(x, y, color='y', label='validation loss')\n",
    "\n",
    "ax1.set(title=\"Training\", \n",
    "      ylabel=\"Accuracy/Loss\", xlabel=\"Epoch\");\n",
    "\n",
    "ax2.set(title=\"Validation\", \n",
    "      ylabel=\"Accuracy/Loss\", xlabel=\"Epoch\");\n",
    "\n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "ax1.legend()\n",
    "ax2.legend() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state\n",
    "torch.save(model.state_dict(), './model-state.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New - Saving the model in onxx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnx onnxscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = Cnn()\n",
    "torch_model.load_state_dict(torch.load('./model-state.pth'))\n",
    "\n",
    "torch_model.eval()\n",
    "\n",
    "torch_input = torch.randn(1, 1, 224, 224)\n",
    "onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)\n",
    "\n",
    "# onnx_program.save(\"my_cats_dogs_cnn_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoADING ONNX Model in memory to check if everythings working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"my_cats_dogs_cnn_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing with Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "url = \"https://i.redd.it/02rmlxw3lpa11.jpg\"\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "# display(img)\n",
    "\n",
    "image_size = 224\n",
    "# Transform this image\n",
    "transform = T.Compose([\n",
    "    T.Resize((image_size, image_size)),\n",
    "    T.Grayscale(),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "img_transformed = transform(img)\n",
    "torch_input = img_transformed.unsqueeze(0)\n",
    "print(img_transformed.shape)\n",
    "print(torch_input.shape)\n",
    "onnx_input = onnx_program.adapt_torch_inputs_to_onnx(torch_input)\n",
    "# print(f\"Input length: {len(onnx_input)}\")\n",
    "# print(f\"Sample input: {onnx_input}\")\n",
    "print(\"torch_input\", torch_input)\n",
    "print(\"onnx_input\", onnx_input)\n",
    "ort_session = onnxruntime.InferenceSession(\"./my_cats_dogs_cnn_model.onnx\", providers=['CPUExecutionProvider'])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
    "\n",
    "# output = session.run(None, {input_name: transformed_img_tensor_unsqueezed.numpy()})[0]\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)\n",
    "\n",
    "model.to(\"cpu\")\n",
    "torch_output = model(torch_input).detach().numpy()[0]\n",
    "\n",
    "print(torch_output)\n",
    "print(onnxruntime_outputs[0][0])\n",
    "\n",
    "# Compare the results to check if they are the similar\n",
    "print(\"outputs are same!\" if torch_output.argmax() == onnxruntime_outputs[0][0].argmax() else \"Not the same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "output = torch_model(torch_input)\n",
    "\n",
    "# Apply softmax to the output\n",
    "pred = F.softmax(output, dim=1)\n",
    "\n",
    "# Get the class with the highest probability\n",
    "output_class_id = pred.argmax(dim=1).detach().numpy()[0]\n",
    "print(output_class_id, pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Run the CNN Model class from above blocks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "new_model = Cnn()\n",
    "new_model.load_state_dict(torch.load(\n",
    "        \"./model-state.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Run the LabelEncoder class from above blocks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url=\"https://images.pexels.com/photos/45201/kitty-cat-kitten-pet-45201.jpeg\"\n",
    "# url=\"https://images.pexels.com/photos/2023384/pexels-photo-2023384.jpeg\"\n",
    "# url=\"https://cdn.pixabay.com/photo/2017/06/20/22/14/man-2425121_960_720.jpg\"\n",
    "# url = \"https://media.geeksforgeeks.org/wp-content/uploads/20210318103632/gfg-300x300.png\"\n",
    "url = \"https://i.redd.it/02rmlxw3lpa11.jpg\"\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "# display(img)\n",
    "\n",
    "image_size = 224\n",
    "# Transform this image\n",
    "transform = T.Compose([\n",
    "    T.Resize((image_size, image_size)),\n",
    "    T.Grayscale(),\n",
    "    # T.ToTensor(),\n",
    "])\n",
    "\n",
    "# inference this image\n",
    "print(\"Transformed Image\")\n",
    "transformed_img = transform(img)\n",
    "display(transformed_img)\n",
    "\n",
    "transformed_img_tensor = T.ToTensor()(transformed_img)\n",
    "\n",
    "print(\"Tensor -> PIL Image\")\n",
    "img_n = T.ToPILImage()(transformed_img_tensor)\n",
    "display(img_n)\n",
    "# Inference this\n",
    "new_model.eval()\n",
    "\n",
    "print(transformed_img_tensor.size())\n",
    "transformed_img_tensor_unsqueezed = transformed_img_tensor.unsqueeze(0)\n",
    "print(transformed_img_tensor_unsqueezed.size())\n",
    "\n",
    "\n",
    "output = new_model(transformed_img_tensor_unsqueezed)\n",
    "\n",
    "# Apply softmax to the output\n",
    "pred = F.softmax(output, dim=1)\n",
    "\n",
    "# Get the class with the highest probability\n",
    "output_class_id = pred.argmax(dim=1).detach().numpy()[0]\n",
    "\n",
    "output_class = label_transformer.decoder(output_class_id)\n",
    "print(\"output_class:\", output_class)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"Its a \", output_class, \"with a probability of \", pred.detach().numpy()[0][output_class_id])\n",
    "print(\"*\"*50)\n",
    "\n",
    "# Map the output class id to the name of the class\n",
    "probs = {}\n",
    "for i in range(len(pred.detach().numpy()[0])):\n",
    "    print(\"Class: \", label_transformer.decoder(i), \"Prob: \", pred.detach().numpy()[0][i])\n",
    "    probs[label_transformer.decoder(i)] = pred.detach().numpy()[0][i]\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "  'https://media.geeksforgeeks.org/wp-content/uploads/20210318103632/gfg-300x300.png',\n",
    "   \"gfg.png\")\n",
    "  \n",
    "img = Image.open(\"gfg.png\")\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# url = \"https://media.geeksforgeeks.org/wp-content/uploads/20210318103632/gfg-300x300.png\"\n",
    "url = \"https://www.inferdo.com/img/label-1.jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thirdparty_image_labels(url):\n",
    "    payload = {\"url\": url}\n",
    "    headers = {\n",
    "      \"content-type\": \"application/json\",\n",
    "      \"X-RapidAPI-Key\": \"c888010467msha782d4c79ee3a6fp110abbjsndc0927e072a6\",\n",
    "      \"X-RapidAPI-Host\": \"image-labeling1.p.rapidapi.com\"\n",
    "    }\n",
    "    response = requests.request(\"POST\", \"https://image-labeling1.p.rapidapi.com/img/label\", headers=headers, json=payload)\n",
    "\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "# url = \"https://i.redd.it/02rmlxw3lpa11.jpg\"\n",
    "# url = \"https://images.pexels.com/photos/45201/kitty-cat-kitten-pet-45201.jpeg\"\n",
    "api_url = \"https://image-labeling1.p.rapidapi.com/img/label\"\n",
    "url = \"https://www.inferdo.com/img/label-1.jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "display(img)\n",
    "\n",
    "get_thirdparty_image_labels(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "884e1fa5afb98415943fc4ac8c0745775853da98530a5437d70c9c2d795d2b7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
